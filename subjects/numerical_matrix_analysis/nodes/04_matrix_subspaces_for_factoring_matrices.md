# Matrix Computations
**Marko Huhtanen**

## 4 Matrix subspaces for factoring matrices (continued)

### Rank-one updates and shifts

**Example 6 (continued)**  
Let $e = (1, 1, \dots, 1)^T \in \mathbb{C}^n$. Then $e\alpha^*$ is a rank-one matrix for any $\alpha \in \mathbb{C}^n$. The $j$-th column of $e\alpha^*$ is $\alpha_j e$. Thus

$$
(e\alpha^*)_{lj} = e_l \alpha_j = \alpha_j \quad \text{for all } l = 1,\dots,n.
$$

This means that the $j$-th column of $e\alpha^*$ consists of the scalar $\alpha_j$ repeated $n$ times. Such matrices appear, e.g., in principal component analysis when one subtracts the mean from data. If the data matrix is $A$, then the mean-corrected matrix is $A - e\alpha^*$ with

$$
\alpha_j = \frac{1}{n} \sum_{l=1}^n a_{lj}.
$$

Often one approximates $A \approx e\alpha^* + F_k$ for small $k$ (i.e., a constant column plus a low-rank correction). This is the essence of many data-analysis techniques.

Matrices of the form $I + uv^*$ ($u,v \in \mathbb{C}^n$) are called **rank-one updates** of the identity. They are extremely important in numerical linear algebra (e.g., in Krylov methods, quasi-Newton methods, etc.).

In the rank-one case, try this out by finding the scalar $\alpha \in \mathbb{C}$ that solves the equation

$$
(I + u_1 v_1^*)(I + \alpha u_1 v_1^*) = I.
$$

The cost is about one inner product. This is an example of a matrix which is very “easy” to invert, i.e., never use standard methods such as the Gaussian elimination with matrices of this form.

### Shifted matrices and polynomial subspaces

Another very common situation is to consider shifts of a matrix:

$$
A - \alpha I, \quad \alpha \in \mathbb{C}.
$$

One often approximates $A \approx \alpha I + F_k$ for some low-rank $F_k$.

**Definition** A set $V \subset \mathbb{C}^{n \times n}$ is a **matrix subspace** over $\mathbb{C}$ (or $\mathbb{R}$) if it is a linear space, i.e., closed under addition and scalar multiplication.

**Example** For the standard eigenvalue problem $Ax = \lambda x$, take

$$
V = \operatorname{span}\{I, A\}.
$$

For the generalized eigenvalue problem $Ax = \lambda Bx$, take

$$
V = \operatorname{span}\{A, B\}.
$$

Such two-dimensional matrix subspaces are called **linear pencils**.

### Similarity and invariants of matrix subspaces

Two matrices $W, V \in \mathbb{C}^{n \times n}$ are **similar** if there exists invertible $X, Y$ such that

$$
W = X V Y^{-1}.
$$

Similarity preserves eigenvalues (with multiplicities), Jordan structure, etc.

A matrix subspace $V$ is **closed under similarity** (or similarity-invariant) if whenever $V \in V$ is invertible, then every matrix similar to $V$ also belongs to $V$.

**Example** The set of all upper triangular matrices is closed under similarity.

**Definition** Let $V \subset \mathbb{C}^{n \times n}$ be a matrix subspace. The set of **invertible elements** of $V$ is

$$
\operatorname{Inv}(V) = \{ V \in V : \det V \neq 0 \}.
$$

$V$ is **nonsingular** if $\operatorname{Inv}(V) = V \setminus \{0\}$ (i.e., only the zero matrix is singular).

If $W = X V Y^{-1}$ with $V \in \operatorname{Inv}(V)$, then clearly $W^{-1} = Y V^{-1} X^{-1} \in V$ whenever $V$ is closed under inversion.

**Definition** The **transpose** of a matrix subspace $V$ is

$$
V^T = \{ V^T : V \in V \}.
$$

### Polynomials of matrices and Krylov subspaces

Let $p(z) = \sum_{j=0}^k c_j z^j$, $c_j \in \mathbb{C}$. Define

$$
p(A) = \sum_{j=0}^k c_j A^j \quad (A^0 = I).
$$

A scalar polynomial $p$ is said to **annihilate** $A$ if $p(A) = 0$.

The **minimal polynomial** of $A$ is the monic polynomial of lowest degree that annihilates $A$.

**Example**

$$
A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}, \quad
A^2 = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}, \quad
A^2 - 2A + I = 0.
$$

So $p(z) = z^2 - 2z + 1 = (z-1)^2$ is the minimal polynomial.

**Cayley–Hamilton Theorem** Let $q(z) = \det(A - zI)$ be the characteristic polynomial of $A \in \mathbb{C}^{n \times n}$. Then $q(A) = 0$.

**Theorem (on matrix polynomials)** Let $p(z) = z^k + c_{k-1} z^{k-1} + \cdots + c_0$ be monic and $p(A) = 0$. Write

$$
p(z) = (z + c_0^{-1} (\text{higher terms})).
$$

If $c_0 \neq 0$, then

$$
A^{-1} = -c_0^{-1} (A^{k-1} + c_{k-1} A^{k-2} + \cdots + c_1 I).
$$

If $c_0 = 0$, then $A$ is singular and

$$
A(A^{k-1} + c_{k-1} A^{k-2} + \cdots + c_1 I) = 0.
$$

**Corollary** If $V$ is a matrix subspace closed under taking inverses (when they exist) and under transposition, and $p$ is any scalar polynomial, then

$$
V \in V \quad \Rightarrow \quad p(V) \in V.
$$

Such subspaces are called **polynomially invariant** or **algebras**.

### Krylov subspaces (of matrices)

**Definition** For $A \in \mathbb{C}^{n \times n}$, the $j$-th **Krylov subspace** (generated by the identity) is

$$
K_j(A; I) = \operatorname{span}\{ I, A, A^2, \dots, A^{j-1} \}, \quad j = 1,2,\dots
$$

Clearly $K_j(A; I) \subset K_{j+1}(A; I)$, and the sequence stabilizes at some point (dimension at most $n$).

If $A = V_1 V_2$ with $V_1, V_2$ from some matrix subspaces and $W$ is invertible, then

$$
A W = V_1 (V_2 W) \in V_1 W,
$$

so one can generate new factorizations from old ones.

### Projections and invariant subspaces

Let $P \in \mathbb{C}^{n \times n}$ be a projection: $P^2 = P$. Then

- $R(P) = \{ y : y = Px \text{ for some } x \}$ is the range (image),
- $R(I-P)$ is the complementary subspace.

A subspace $S$ is **invariant** under $A$ if $A S \subset S$. Equivalently, if $q_1,\dots,q_k$ form a basis of $S$, then $A [q_1 \cdots q_k]$ has its columns in $S$.

**Problem** Show that $R(P)$ is $A$-invariant if and only if $A P = P A$.
