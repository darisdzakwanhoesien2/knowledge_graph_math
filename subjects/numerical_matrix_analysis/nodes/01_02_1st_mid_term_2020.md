# 031051S Numerical Matrix Analysis, the 1st partial exam

## Problem 1

Let 

$$
A = \begin{bmatrix}
\sqrt{3} & 2 \\
0 & \sqrt{3}
\end{bmatrix}.
$$

Give the best rank-1 approximation to $A$.

### Solution

The best rank-1 approximation is given by the first term of the SVD: $\sigma_1 u_1 v_1^*$.

Compute the singular values of $A$:

$$
A^*A = \begin{bmatrix}
3 & 2\sqrt{3} \\
2\sqrt{3} & 7
\end{bmatrix},
\qquad
\det(A^*A - \lambda I) = \lambda^2 - 10\lambda + 3 = 0
\;\Rightarrow\;
\lambda = 5 \pm 2\sqrt{6}.
$$

Thus $\sigma_1 = \sqrt{5 + 2\sqrt{6}}$, $\sigma_2 = \sqrt{5 - 2\sqrt{6}}$.

The best rank-1 approximation is therefore

$$
A_1 = \sigma_1 u_1 v_1^*
$$

where $u_1$ is the left singular vector corresponding to $\sigma_1^2$ and $v_1$ the right singular vector.

(Alternatively, one can compute it explicitly, but the answer is the outer product of the first left and right singular vectors scaled by $\sigma_1$.)

## Problem 2

Let 

$$
A = \begin{bmatrix}
\sqrt{3} & 2 \\
0 & \sqrt{3}
\end{bmatrix}.
$$

Factor $A$ into the product of two symmetric matrices. How many such factorizations are there?

### Solution

Since $A$ is already upper triangular and real, one trivial factorization is $A = I \cdot A$ (both symmetric? No, $A$ is not symmetric).

A symmetric-symmetric factorization is of the form $A = S T$ with $S^T = S$, $T^T = T$.

Because $A$ has equal diagonal entries, it is normal ($A^T A = A A^T$), so it can be diagonalized orthogonally:

$$
A = Q \Sigma Q^T, \quad \Sigma = \operatorname{diag}(\sqrt{3},\sqrt{3}).
$$

Thus $A = (Q \Sigma^{1/2}) (\Sigma^{1/2} Q^T)$ and both factors are symmetric positive semidefinite.

More generally, any factorization $A = S T$ with $S,T$ symmetric is of the form

$$
S = U D^{1/2}, \quad T = D^{1/2} U^T
$$

for orthogonal $U$ and diagonal $D$ with nonnegative entries and $\det D = (\det A)^2 = 3$ (or one factor singular if we allow it, but the problem probably wants invertible factors).

Since the eigenvalues are equal, the orthogonal matrices that diagonalize $A$ are not unique (any rotation in the plane works).

The number of essentially different symmetric-symmetric factorizations (up to scaling one factor by a scalar and the other by its inverse) is infinite, because the eigenspace is the whole $\mathbb{R}^2$.

A simple explicit example:

$$
A = \begin{bmatrix}
1 & 1 \\
1 & 2
\end{bmatrix}
\begin{bmatrix}
2 & 1 \\
1 & 1
\end{bmatrix}
$$

(check: both symmetric, product $A$).

There are infinitely many such factorizations.

## Problem 3

Let 

$$
A = \begin{bmatrix}
4 & 4 & 2 \\
4 & 4 & -2 \\
2 & 0 & 2
\end{bmatrix}.
$$

Compute the partially pivoted LU factorization of $A$. Is $A$ invertible? (Give careful arguments.)

### Solution

Perform Gaussian elimination with partial pivoting (choose largest entry in column as pivot).

Column 1: entries 4,4,2 → largest in absolute value 4 (two choices, rows 1 and 2). Take row 1 as pivot (or row 2, symmetric).

Swap not needed.

Eliminate:

Row2 ← Row2 − Row1  
Row3 ← Row3 − (1/2) Row1

Result:

$$
\begin{bmatrix}
4 & 4 & 2 \\
0 & 0 & -4 \\
0 & -2 & 1
\end{bmatrix}.
$$

Column 2 (from row 2 down): entries 0, -2 → pivot row 3 (value -2).

Swap rows 2 and 3:

$$
\begin{bmatrix}
4 & 4 & 2 \\
0 & -2 & 1 \\
0 & 0 & -4
\end{bmatrix}.
$$

Eliminate below pivot (nothing to do).

Now column 3: only one entry -4 ≠ 0.

Thus

$$
PA = LU
\qquad\text{with}\quad
P = \begin{bmatrix} 
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0 \end{bmatrix},
$$

$$
L = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 1
\end{bmatrix},
\qquad
U = \begin{bmatrix}
4 & 4 & 2 \\
0 & -2 & 1 \\
0 & 0 & -4
\end{bmatrix}
$$

(or equivalent with different row ordering in the first step).

All pivots nonzero → $A$ is invertible (nonsingular).

## Problem 4

Let 

$$
A = \begin{bmatrix}
4 & 4 \\
4 & 5
\end{bmatrix}.
$$

Compute the Cholesky factorization of $A$.

### Solution

$A$ is symmetric positive definite (eigenvalues 1 and 8, or trace 9, det 4 > 0).

Standard Cholesky (without pivoting, possible because positive definite):

$$
A = \begin{bmatrix}
r_{11} & 0 \\
r_{21} & r_{22}
\end{bmatrix}
\begin{bmatrix}
r_{11} & r_{21} \\
0 & r_{22}
\end{bmatrix}.
$$

Solve:

$r_{11}^2 = 4$ → $r_{11} = 2$ (positive by convention),

$r_{21} r_{11} = 4$ → $r_{21} = 2$,

$r_{21}^2 + r_{22}^2 = 5$ → $4 + r_{22}^2 = 5$ → $r_{22} = 1$.

Thus

$$
A = \begin{bmatrix}
2 & 0 \\
2 & 1
\end{bmatrix}
\begin{bmatrix}
2 & 2 \\
0 & 1
\end{bmatrix}.
$$

(Or the transpose form $LL^T$, depending on convention; both are accepted.)